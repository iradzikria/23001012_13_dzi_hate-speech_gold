{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30e2651e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [02/Oct/2023 21:49:57] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/Oct/2023 21:50:02] \"GET /docs/ HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/Oct/2023 21:50:02] \"GET /flasgger_static/swagger-ui.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [02/Oct/2023 21:50:02] \"GET /flasgger_static/lib/jquery.min.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [02/Oct/2023 21:50:02] \"GET /flasgger_static/swagger-ui-standalone-preset.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [02/Oct/2023 21:50:03] \"GET /flasgger_static/swagger-ui-bundle.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [02/Oct/2023 21:50:03] \"GET /docs.json HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# IMPORT LIBRARIES FOR REGEX, PANDAS, NUMPY, SQLITE3, MATPLOTLIB, SEABORN, AND WARNINGS (TO IGNORE VISUALIZATION RESULT WARNING\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# IMPORT LIBRARY FOR FLASK AND SWAGGER\n",
    "from flask import Flask, jsonify, request\n",
    "from flasgger import Swagger, LazyString, LazyJSONEncoder\n",
    "from flasgger import swag_from\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# DEFAULT FLASK AND SWAGGER DEFAULT SETTING\n",
    "app = Flask(__name__)\n",
    "app.json_encoder = LazyJSONEncoder\n",
    "swagger_template = dict(\n",
    "info = {\n",
    "    'title': LazyString(lambda: 'API Documentation for Data Processing and Modeling'),\n",
    "    'version': LazyString(lambda: '1.0.0'),\n",
    "    'description': LazyString(lambda: 'Dokumentasi API untuk Data Processing dan Modeling'),\n",
    "    },\n",
    "    host = LazyString(lambda: request.host)\n",
    ")\n",
    "swagger_config = {\n",
    "    \"headers\": [],\n",
    "    \"specs\": [\n",
    "        {\n",
    "            \"endpoint\": 'docs',\n",
    "            \"route\": '/docs.json',\n",
    "        }\n",
    "    ],\n",
    "    \"static_url_path\": \"/flasgger_static\",\n",
    "    \"swagger_ui\": True,\n",
    "    \"specs_route\": \"/docs/\"\n",
    "}\n",
    "swagger = Swagger(app, template=swagger_template,             \n",
    "                  config=swagger_config)\n",
    "\n",
    "# IMPORT ABUSIVE.CSV AND NEW_KAMUSALAY.CSV\n",
    "df_abusive = pd.read_csv('abusive.csv')\n",
    "abusive = df_abusive.to_numpy()\n",
    "\n",
    "df_alay = pd.read_csv('new_kamusalay.csv', encoding='latin-1', header=None)\n",
    "df_alay.columns = [\"alay\",\"arti\"]\n",
    "alay = df_alay[\"alay\"].values.tolist()\n",
    "arti = df_alay[\"arti\"].values.tolist()\n",
    "\n",
    "# DEFINE ENDPOINTS: BASIC GET\n",
    "@swag_from(\"C:/Users/ACER/Binar DSC/docs/hello_world.yml\", methods=['GET'])\n",
    "@app.route('/', methods=['GET'])\n",
    "def hello_world():\n",
    "    json_response = {\n",
    "        'status_code': 200,\n",
    "        'description': \"Menyapa Hello World\",\n",
    "        'data': \"Hello World\",\n",
    "    }\n",
    "    response_data = jsonify(json_response)\n",
    "    return response_data\n",
    "\n",
    "# DEFINE ENDPOINTS: POST FOR TEXT PROCESSING FROM TEXT INPUT\n",
    "@swag_from(\"C:/Users/ACER/Binar DSC/docs/text_processing.yml\", methods=['POST'])\n",
    "@app.route('/text-processing', methods=['POST'])\n",
    "def text_processing():\n",
    "    \n",
    "    text = request.form.get('text')\n",
    "    \n",
    "    json_response = {\n",
    "        'status_code': 200,\n",
    "        'description': \"Teks yang sudah diproses\",\n",
    "        'data': re.sub(r'[^a-zA-Z0-9]',' ', text)\n",
    "    }\n",
    "    \n",
    "    response_data = jsonify(json_response)\n",
    "    return response_data\n",
    "\n",
    "# DEFINE ENDPOINTS: POST FOR TEXT PROCESSING FROM FILE\n",
    "@swag_from(\"C:/Users/ACER/Binar DSC/docs/text_processing_file.yml\", methods=['POST'])\n",
    "@app.route('/text-processing-file', methods=['POST'])\n",
    "def text_processing_file():\n",
    "    # USING REQUEST TO GET FILE THAT HAS BEEN POSTED FROM API ENDPOINT\n",
    "    file = request.files.get('file')\n",
    "    \n",
    "    # IMPORT FILE OBJECT\n",
    "    df_tweet = pd.read_csv(file, encoding='latin-1').drop_duplicates().dropna()\n",
    "    \n",
    "    # DATA CLEANSING\n",
    "    tweet = df_tweet['Tweet'].str.replace(r'http\\S+|www.\\S+', '', regex=True) #remove url\n",
    "    tweet = tweet.str.replace(r'\\\\x[0-9a-z]{2}', '', regex=True) #remove emoticon\n",
    "    tweet = tweet.str.replace(r'\\\\n', '', regex=True) #remove newline\n",
    "    tweet = tweet.str.replace(r'&amp;', '', regex=True) #remove ampersand\n",
    "    tweet = tweet.str.replace(r'[^a-zA-Z0-9.,!?;/ ]', '', regex=True)\n",
    "    tweet = tweet.values.tolist()\n",
    "\n",
    "    tweet_lingo = [\"USER\", \"user\", \"URL\", \"url\", \"RT\"]\n",
    "\n",
    "    for i in range(len(tweet)):\n",
    "        for word in tweet[i].split():\n",
    "            if word in tweet_lingo:\n",
    "                tweet[i] = tweet[i].replace(word, \"\")\n",
    "    \n",
    "    # SUBSTITUTE ALAY WORDS WITH THEIR MEANINGS\n",
    "    for i in range(len(tweet)):\n",
    "        sentence = tweet[i].lower()\n",
    "\n",
    "        for j in range(len(sentence)):\n",
    "            for k in range(len(alay)):\n",
    "                for word in (re.split('[.,!?;/ ]', sentence)):\n",
    "                    if word == alay[k]:\n",
    "                        sentence = sentence.replace(word, arti[k])\n",
    "\n",
    "        tweet[i] = ''.join(sentence)\n",
    "    \n",
    "    count_abusive = [] * len(tweet)\n",
    "    tweet_type = [] * len(tweet)\n",
    "\n",
    "    # REMOVE ABUSIVE WORDS\n",
    "    for a in range(len(tweet)):\n",
    "        b = 0\n",
    "        for word in (re.split('[.,!?;/ ]', tweet[a])):\n",
    "            if word in abusive:\n",
    "                b = b+1\n",
    "                tweet[a] = tweet[a].replace(word, \"*\" * len(word))\n",
    "        count_abusive.append(b) #count total abusive words in tweet\n",
    "        # SEPARATE TWEET BY IF THERE ARE ABUSIVE WORDS OR NOT\n",
    "        if b > 0:\n",
    "            tweet_type.append('Abusive')\n",
    "        elif b == 0:\n",
    "            tweet_type.append('Not Abusive')\n",
    "    \n",
    "    # TOTAL_CHAR = TOTAL CHARACTERS IN TWEET BEFORE CLEANSING | TOTAL_WORDS = TOTAL WORDS IN TWEET BEFORE CLEANSING\n",
    "    df_tweet['total_char'] = df_tweet['Tweet'].apply(len)\n",
    "    df_tweet['total_words'] = df_tweet['Tweet'].apply(lambda x: len(x.split()))\n",
    "    \n",
    "    # TWEET AFTER CLEANSING\n",
    "    df_tweet['cleaned_tweet'] = pd.DataFrame(tweet)\n",
    "    \n",
    "    # TOTAL_CHAR2 = TOTAL CHARACTERS IN TWEET AFTER CLEANSING | TOTAL_WORDS2 = TOTAL WORDS IN TWEET AFTER CLEANSING\n",
    "    df_tweet['total_char2'] = df_tweet['cleaned_tweet'].apply(len)\n",
    "    df_tweet['total_words2'] = df_tweet['cleaned_tweet'].apply(lambda x: len(x.split()))\n",
    "    \n",
    "    # APPLY THE FUNCTION TO COUNT ABUSIVE WORDS, AND CREATE A NEW COLUMN BASED OFF OF IT\n",
    "    df_tweet['count_abusive'] = pd.DataFrame(count_abusive)\n",
    "    df_tweet['tweet_type'] = pd.DataFrame(tweet_type)\n",
    "    \n",
    "    # CONNECT / CREATE NEW DATABASE AND CREATE NEW TABLE CONSISTING LISTED TABLES\n",
    "    conn = sqlite3.connect('database_project.db')\n",
    "    q_create_table = \"\"\"\n",
    "    create table if not exists df_tweet (Tweet varchar(255), total_char int, total_words int, cleaned_tweet varchar(255), total_char2 int, total_words2 int, tweet_type varchar(255));\n",
    "    \"\"\"\n",
    "    conn.execute(q_create_table)\n",
    "    conn.commit()\n",
    "    \n",
    "    # CHECK WHETHER TABLE ALREADY HAS DATA IN IT (TABLE HAS ROWS OF DATA IN IT)\n",
    "    cursor = conn.execute(\"select count(*) from df_tweet\")\n",
    "    num_rows = cursor.fetchall()\n",
    "    num_rows = num_rows[0][0]\n",
    "    \n",
    "    #  DO DATA INSERTIONS IF TABLE HAS NO DATA IN IT    \n",
    "    if num_rows == 0:\n",
    "    # DO ITERATIONS TO INSERT DATA (EACH ROW) FROM FINAL DATAFRAME (df_tweet)\n",
    "        for i in range(len(df_tweet)):\n",
    "            tweet = df_tweet['Tweet'].iloc[i]\n",
    "            total_char = int(df_tweet['total_char'].iloc[i])\n",
    "            total_words = int(df_tweet['total_words'].iloc[i])\n",
    "            cleaned_tweet = df_tweet['cleaned_tweet'].iloc[i]\n",
    "            total_char2 = int(df_tweet['total_char2'].iloc[i])\n",
    "            total_words2 = int(df_tweet['total_words2'].iloc[i])\n",
    "            tweet_type = df_tweet['tweet_type'].iloc[i]\n",
    "    \n",
    "            q_insertion = \"insert into df_tweet (Tweet, total_char, total_words, cleaned_tweet, total_char2, total_words2, tweet_type) values (?,?,?,?,?,?,?)\"\n",
    "            conn.execute(q_insertion,(tweet,total_char,total_words,cleaned_tweet,total_char2,total_words2,tweet_type))\n",
    "            conn.commit()    \n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    # VISUALIZE THE PERCENTAGE OF ABUSIVE TWEETS\n",
    "    plt.figure()\n",
    "    df_tweet.groupby('tweet_type').tweet_type.count().plot(kind =\"pie\", title='Persentase Abusive Tweets', autopct='%.2f%%', labels=None, ylabel='', legend=True)\n",
    "    plt.savefig('pie_abusive.jpeg')\n",
    "    \n",
    "    # VISUALIZE THE NUMBER OF ABUSIVE WORDS\n",
    "    plt.figure()\n",
    "    countplot = sns.countplot(data=df_tweet, x=\"count_abusive\")\n",
    "    plt.title('Count of Estimated Number of Abusive Words')\n",
    "    plt.xlabel('Estimated Number of Abusive Words')\n",
    "    plt.savefig('new_countplot.jpeg')\n",
    "    \n",
    "    # VISUALIZE THE NUMBER OF WORDS\n",
    "    plt.figure()\n",
    "    boxplot = sns.boxplot(data=df_tweet, x=\"total_words2\")\n",
    "    plt.title('Number of Words Boxplot (after tweet cleansing)')\n",
    "    plt.xlabel('')\n",
    "    plt.savefig('boxplot_total_words.jpeg')\n",
    "    \n",
    "    # VISUALIZE WORDCLOUD\n",
    "    plt.figure()\n",
    "    text = ' '.join(df_tweet['cleaned_tweet'])\n",
    "    wordcloud = WordCloud(collocations=False).generate(text)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig('wordcloud.jpeg')\n",
    "    \n",
    "    # VISUALIZE SCATTERPLOT\n",
    "    plt.figure()\n",
    "    sns.scatterplot(data=df_tweet, x='total_char2', y='total_words2')\n",
    "    plt.savefig('scatter,jpeg')\n",
    "    \n",
    "    # OUTPUT THE RESULT IN JSON FORMAT\n",
    "    json_response = {\n",
    "        'status_code': 200,\n",
    "        'description': \"Teks yang sudah diproses\",\n",
    "        'data': list(df_tweet['cleaned_tweet'])\n",
    "    }\n",
    "    \n",
    "    response_data = jsonify(json_response)\n",
    "    return response_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328747d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
